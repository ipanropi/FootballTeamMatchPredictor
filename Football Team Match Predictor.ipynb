{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd8be1e",
   "metadata": {},
   "source": [
    "# Predicting Manchester United's Match Outcomes: A Machine Learning Approach\n",
    "\n",
    "This project is about creating a computer program to predict the results of Manchester United's soccer games in the English Premier League. First, it collects past game data from websites, cleans it up, and then uses charts to understand the data better. The data is divided into two parts: one for training the computer program and the other for testing it. The program uses TensorFlow, a tool for building machine learning models, to learn from the training data. After training, the program is tested to see how well it can predict game outcomes. In the end, this trained program is saved for future use in guessing the results of Manchester United's games, showing how computer science can be used in sports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649317e",
   "metadata": {},
   "source": [
    "# Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b08ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fe204",
   "metadata": {},
   "source": [
    "# Scraping Data and Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = np.array([\n",
    "    'https://fbref.com/en/squads/19538871/2023-2024/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2022-2023/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2021-2022/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2020-2021/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2019-2020/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2018-2019/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League',\n",
    "    'https://fbref.com/en/squads/19538871/2017-2018/matchlogs/c9/schedule/Manchester-United-Scores-and-Fixtures-Premier-League'])\n",
    "\n",
    "url2 = np.array([\n",
    "    'https://fbref.com/en/comps/9/Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2022-2023/2022-2023-Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2021-2022/2021-2022-Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2020-2021/2020-2021-Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2019-2020/2019-2020-Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2018-2019/2018-2019-Premier-League-Stats',\n",
    "    'https://fbref.com/en/comps/9/2017-2018/2017-2018-Premier-League-Stats'])\n",
    "\n",
    "df_matches = pd.DataFrame()\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(url1)):\n",
    "    url_1 = url1[i]\n",
    "    url_2 = url2[i]\n",
    "    \n",
    "    # Send an HTTP GET request to the URL1\n",
    "    response1 = requests.get(url_1)\n",
    "    \n",
    "    # Check if the request to URL1 was successful (status code 200)\n",
    "    if response1.status_code == 200:\n",
    "        # Use pandas to read HTML tables from the response\n",
    "        tables1 = pd.read_html(response1.text)\n",
    "\n",
    "        # Assuming the table you want is the first one on the page\n",
    "        if len(tables1) > 0:\n",
    "            df_matches = tables1[0][['Venue', 'Result', 'Opponent', 'Attendance']].dropna()\n",
    "            \n",
    "        else:\n",
    "            print(f\"No tables found on the webpage {url_1}.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage {url_1}. Status code: {response1.status_code}\")\n",
    "\n",
    "    # Send an HTTP GET request to the URL2\n",
    "    response2 = requests.get(url_2)\n",
    "    \n",
    "    # Check if the request to URL2 was successful (status code 200)\n",
    "    if response2.status_code == 200:\n",
    "        # Use pandas to read HTML tables from the response\n",
    "        tables2 = pd.read_html(response2.text)\n",
    "\n",
    "        # Assuming the table you want is the third one on the page\n",
    "        if len(tables2) > 2:\n",
    "            df_strength = tables2[2]\n",
    "            \n",
    "            df_strength.columns = df_strength.columns.get_level_values(1)\n",
    "            df_strength = df_strength.iloc[:, [0] + list(range(22, df_strength.shape[1]))]\n",
    "            df_oppstrength = df_strength.add_suffix('_Opp')\n",
    "            df_oppstrength = df_oppstrength.rename(columns={'Squad_Opp': 'Opponent'})\n",
    "\n",
    "            df_MUStrength = df_strength.loc[df_strength['Squad'] == 'Manchester Utd'].drop(columns=['Squad'], axis=1).add_suffix('_MU')\n",
    "            \n",
    "            \n",
    "            \n",
    "            df_merged = pd.merge(df_oppstrength, df_matches, on='Opponent', how='inner')\n",
    "            df_merged = pd.concat([pd.concat([df_MUStrength]*len(df_merged), ignore_index=True), df_merged], axis=1)\n",
    "            df_merged.columns = [re.sub(r'[^A-Za-z0-9_.\\\\/>-]', '_', col) for col in df_merged.columns]\n",
    "            \n",
    "            result_df = pd.concat([result_df, df_merged],ignore_index=True)\n",
    "            print(result_df)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No tables found on the webpage {url_2}.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage {url_2}. Status code: {response2.status_code}\")\n",
    "\n",
    "result_mapping = {'W': 2, 'L': 0, 'D': 1}\n",
    "\n",
    "# Replace values in the \"Result\" column\n",
    "result_df['Result'] = result_df['Result'].replace(result_mapping)\n",
    "\n",
    "result_df['Result'] = pd.to_numeric(result_df['Result'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c394a5e",
   "metadata": {},
   "source": [
    "# Visualising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2667ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(result_df.drop(columns=['Opponent', 'Venue']).corr(), cmap='YlGnBu', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cb5ff",
   "metadata": {},
   "source": [
    "# Splitting the Data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df.drop(columns=['Opponent', 'Attendance'])\n",
    "y = result_df['Result']\n",
    "X = result_df.drop(columns=['Result'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = tf.one_hot(y_train, depth=3)\n",
    "y_test = tf.one_hot(y_test, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_dict = {name: np.array(value) for name, value in X_train.items()}\n",
    "X_test_features_dict = {name: np.array(value) for name, value in X_test.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ds = tf.data.Dataset.from_tensor_slices((X_train_features_dict, y_train))\n",
    "X_test_ds = tf.data.Dataset.from_tensor_slices((X_test_features_dict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345328e",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "for name, column in X_train.items():\n",
    "    dtype= column.dtype\n",
    "    if dtype == object:\n",
    "        dtype = tf.string\n",
    "    else:\n",
    "        dtype = tf.float32\n",
    "    \n",
    "    inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_inputs = {name:input for name,input in inputs.items() if input.dtype == tf.float32}\n",
    "\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(X_train[numeric_inputs.keys()]))\n",
    "all_numeric_inputs = norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed170e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_inputs = [all_numeric_inputs]\n",
    "preprocessed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb844a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, input in inputs.items():\n",
    "    if input.dtype == tf.float32:\n",
    "        continue\n",
    "    \n",
    "    lookup = layers.StringLookup(vocabulary=np.unique(X_train[name]))\n",
    "    one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "    \n",
    "    x = lookup(input)\n",
    "    x = one_hot(x)\n",
    "    preprocessed_inputs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b83b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "X_train_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d210834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_dict = {name: np.array(value) for name, value in X_train.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83defc",
   "metadata": {},
   "source": [
    "# Creating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea26bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epl_model(preprocessing_head, inputs):\n",
    "  body = tf.keras.Sequential([\n",
    "    layers.Dense(32, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, kernel_regularizer=regularizers.l2(0.001), activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  preprocessed_inputs = preprocessing_head(inputs)\n",
    "  result = body(preprocessed_inputs)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "  print(inputs)\n",
    "  print(result)\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "epl_model = epl_model(X_train_preprocessing, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afde70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_batches = X_train_ds.shuffle(len(y_train)).batch(16)\n",
    "X_test_batches = X_test_ds.shuffle(len(y_test)).batch(16)\n",
    "X_test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33e1e0",
   "metadata": {},
   "source": [
    "# Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f87b1ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epl_model.fit(X_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d15804",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = epl_model.evaluate(X_test_batches, verbose=0)\n",
    "print('test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0fdaa1",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epl_model.save('MUMatchPredictor_Latest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
